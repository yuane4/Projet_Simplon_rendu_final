{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement sur une répartition 50 -50, 1 millions de lignes et dataset amélioré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ad = pd.read_excel('adresses_50_50_am.xlsx', encoding='latin-1')\n",
    "df_bruit = pd.read_excel('bruit.xlsx', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50_50 = pd.concat([df_ad, df_bruit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_50_50=sklearn.utils.shuffle(df_50_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((683004,), (336406,), (683004,), (336406,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "#on divise l'échantillon en train et test \n",
    "X = df_50_50['Adresses']\n",
    "y = df_50_50['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((683004, 279655), (336406, 279655))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "X_train_tf = tfidf_vect.fit_transform(X_train.astype('U'))\n",
    "vectorize = tfidf_vect\n",
    "pickle.dump(vectorize,open(\"vectorizer.pkl\",\"wb\"))\n",
    "X_test_tf = tfidf_vect.transform(X_test.astype('U'))\n",
    "X_train_tf.shape, X_test_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<683004x279655 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 2877230 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Modèle numéro 1 : Naïve Bayes multinomial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha = 0.01, fit_prior = False)\n",
    "clf = clf.fit(X_train_tf, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       bruit       1.00      0.94      0.97    165799\n",
      "     adresse       0.95      1.00      0.97    170607\n",
      "\n",
      "    accuracy                           0.97    336406\n",
      "   macro avg       0.97      0.97      0.97    336406\n",
      "weighted avg       0.97      0.97      0.97    336406\n",
      "\n",
      "accuracy of the model is : 0.9706307259680267\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics #on imprime les métrics d'évaluations\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target_names = ['bruit', 'adresse']\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(\"accuracy of the model is :\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[156080,   9719],\n",
       "       [   161, 170446]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.9709503897488155\n",
      "Best parameters set:\n",
      "\talpha: 0.01\n",
      "\tfit_prior: False\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "X = X_train_tf\n",
    "y = y_train\n",
    "parameters = {'alpha': (3,2, 1.3, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001), 'fit_prior' : (True, False)}\n",
    "gs_clf = GridSearchCV(MultinomialNB(), parameters, cv=8, n_jobs=-1)\n",
    "gs_clf.fit(X, y)  \n",
    "#print(\"done in {0}s\".format(time() - t0))  \n",
    "print(\"Best score: {0}\".format(gs_clf.best_score_))  \n",
    "print(\"Best parameters set:\")  \n",
    "best_parameters = gs_clf.best_estimator_.get_params()\n",
    "for param_name in sorted(list(parameters.keys())):  \n",
    "            print(\"\\t{0}: {1}\".format(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.97140883 0.97166651 0.97074119 0.97156109 0.9709634  0.97016691\n",
      " 0.97028404 0.97081113]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(MultinomialNB(alpha = 0.01, fit_prior = False), X, y, cv=8)\n",
    "print(\"Cross-validation scores: {}\".format(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Modèle numéro 2 : Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Lr = LogisticRegression(random_state=0, max_iter = 200, solver = 'lbfgs', class_weight = 'dict', C = 1.2)\n",
    "Lr = Lr.fit(X_train_tf, y_train)\n",
    "y_pred = Lr.predict(X_test_tf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       bruit       0.99      1.00      0.99    165799\n",
      "     adresse       1.00      0.99      0.99    170607\n",
      "\n",
      "    accuracy                           0.99    336406\n",
      "   macro avg       0.99      0.99      0.99    336406\n",
      "weighted avg       0.99      0.99      0.99    336406\n",
      "\n",
      "accuracy of the model is : 0.9931957218361147\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics #on imprime les métrics d'évaluations\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target_names = ['bruit', 'adresse']\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(\"accuracy of the model is :\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Modèle numéro 3 : Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "Rf = RandomForestClassifier(max_depth=8, random_state=0, n_estimators = 350, bootstrap = True, max_features = 'auto', min_samples_split = 5)\n",
    "Rf = Rf.fit(X_train_tf, y_train)\n",
    "y_pred = Rf.predict(X_test_tf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       bruit       0.96      1.00      0.98    165799\n",
      "     adresse       1.00      0.96      0.98    170607\n",
      "\n",
      "    accuracy                           0.98    336406\n",
      "   macro avg       0.98      0.98      0.98    336406\n",
      "weighted avg       0.98      0.98      0.98    336406\n",
      "\n",
      "accuracy of the model is : 0.9758387186911054\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics #on imprime les métrics d'évaluations\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target_names = ['bruit', 'adresse']\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(\"accuracy of the model is :\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Modèle numéro 4 : gradient Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=8, random_state=0)\n",
    "gb = gb.fit(X_train_tf, y_train)\n",
    "y_pred = gb.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       bruit       0.99      1.00      0.99    165799\n",
      "     adresse       1.00      0.99      0.99    170607\n",
      "\n",
      "    accuracy                           0.99    336406\n",
      "   macro avg       0.99      0.99      0.99    336406\n",
      "weighted avg       0.99      0.99      0.99    336406\n",
      "\n",
      "accuracy of the model is : 0.9929906125336647\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics #on imprime les métrics d'évaluations\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target_names = ['bruit', 'adresse']\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=target_names))\n",
    "print(\"accuracy of the model is :\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Une fois les modèle entrainé, testé puis amélioré on peut les enregistrer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# pickle.dump(clf, open('nbm.pkl', 'wb'))\n",
    "# pickle.dump(Lr, open('Lr.pkl', 'wb'))\n",
    "# pickle.dump(Rf, open('Rf.pkl', 'wb'))\n",
    "pickle.dump(gb, open('gb.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test sur deux répartitions biaisées "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_991 = pd.read_excel('99-1.xlsx', encoding='latin-1')\n",
    "df_199 = pd.read_excel('1-99.xlsx', encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Répartition 99% d'adresse vs 1% de bruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = df_991['Adresses']\n",
    "y = df_991['Target']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100001, 279655), (100001,))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "vectorisation = pickle.load(open(\"vectorizer.pkl\", \"rb\"))\n",
    "X_tf = vectorisation.transform(X.astype('U'))\n",
    "\n",
    "X_tf.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load the model from disk\n",
    "\n",
    "nbm = pickle.load(open(\"nbm.pkl\", \"rb\"))\n",
    "y_pred = nbm.predict(X_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       bruit       0.64      0.78      0.70      1002\n",
      "     adresse       1.00      1.00      1.00     98999\n",
      "\n",
      "    accuracy                           0.99    100001\n",
      "   macro avg       0.82      0.89      0.85    100001\n",
      "weighted avg       0.99      0.99      0.99    100001\n",
      "\n",
      "accuracy of the model is : 0.993360066399336\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics #on imprime les métrics d'évaluations\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target_names = ['bruit', 'adresse']\n",
    "print(metrics.classification_report(y, y_pred, target_names=target_names))\n",
    "print(\"accuracy of the model is :\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load the model from disk\n",
    "\n",
    "lr = pickle.load(open(\"Lr.pkl\", \"rb\"))\n",
    "y_pred = lr.predict(X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       bruit       0.27      1.00      0.42      1002\n",
      "     adresse       1.00      0.97      0.99     98999\n",
      "\n",
      "    accuracy                           0.97    100001\n",
      "   macro avg       0.63      0.99      0.70    100001\n",
      "weighted avg       0.99      0.97      0.98    100001\n",
      "\n",
      "accuracy of the model is : 0.972370276297237\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics #on imprime les métrics d'évaluations\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target_names = ['bruit', 'adresse']\n",
    "print(metrics.classification_report(y, y_pred, target_names=target_names))\n",
    "print(\"accuracy of the model is :\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load the model from disk\n",
    "\n",
    "rf = pickle.load(open(\"Rf.pkl\", \"rb\"))\n",
    "y_pred = rf.predict(X_tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       bruit       0.11      1.00      0.19      1002\n",
      "     adresse       1.00      0.92      0.96     98999\n",
      "\n",
      "    accuracy                           0.92    100001\n",
      "   macro avg       0.55      0.96      0.57    100001\n",
      "weighted avg       0.99      0.92      0.95    100001\n",
      "\n",
      "accuracy of the model is : 0.9159008409915901\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics #on imprime les métrics d'évaluations\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target_names = ['bruit', 'adresse']\n",
    "print(metrics.classification_report(y, y_pred, target_names=target_names))\n",
    "print(\"accuracy of the model is :\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load the model from disk\n",
    "\n",
    "gb = pickle.load(open(\"gb.pkl\", \"rb\"))\n",
    "y_pred = gb.predict(X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       bruit       0.24      1.00      0.39      1002\n",
      "     adresse       1.00      0.97      0.98     98999\n",
      "\n",
      "    accuracy                           0.97    100001\n",
      "   macro avg       0.62      0.98      0.69    100001\n",
      "weighted avg       0.99      0.97      0.98    100001\n",
      "\n",
      "accuracy of the model is : 0.9684403155968441\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics #on imprime les métrics d'évaluations\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target_names = ['bruit', 'adresse']\n",
    "print(metrics.classification_report(y, y_pred, target_names=target_names))\n",
    "print(\"accuracy of the model is :\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Répartition 1% d'adresse vs 99% de bruit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_991\n",
    "# df_199\n",
    "\n",
    "X = df_199['Adresses']\n",
    "y = df_199['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((99999, 279655), (99999,))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "vectorisation = pickle.load(open(\"vectorizer.pkl\", \"rb\"))\n",
    "X_tf199 = vectorisation.transform(X.astype('U'))\n",
    "\n",
    "\n",
    "X_tf199.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load the model from disk\n",
    "\n",
    "nbm = pickle.load(open(\"nbm.pkl\", \"rb\"))\n",
    "y_pred = nbm.predict(X_tf199)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       bruit       1.00      0.76      0.87     98999\n",
      "     adresse       0.04      1.00      0.08      1000\n",
      "\n",
      "    accuracy                           0.77     99999\n",
      "   macro avg       0.52      0.88      0.47     99999\n",
      "weighted avg       0.99      0.77      0.86     99999\n",
      "\n",
      "accuracy of the model is : 0.7658176581765818\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics #on imprime les métrics d'évaluations\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target_names = ['bruit', 'adresse']\n",
    "print(metrics.classification_report(y, y_pred, target_names=target_names))\n",
    "print(\"accuracy of the model is :\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load the model from disk\n",
    "\n",
    "lr = pickle.load(open(\"Lr.pkl\", \"rb\"))\n",
    "y_pred = lr.predict(X_tf199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       bruit       1.00      0.98      0.99     98999\n",
      "     adresse       0.33      0.96      0.49      1000\n",
      "\n",
      "    accuracy                           0.98     99999\n",
      "   macro avg       0.66      0.97      0.74     99999\n",
      "weighted avg       0.99      0.98      0.98     99999\n",
      "\n",
      "accuracy of the model is : 0.979839798397984\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics #on imprime les métrics d'évaluations\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target_names = ['bruit', 'adresse']\n",
    "print(metrics.classification_report(y, y_pred, target_names=target_names))\n",
    "print(\"accuracy of the model is :\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load the model from disk\n",
    "\n",
    "rf = pickle.load(open(\"Rf.pkl\", \"rb\"))\n",
    "y_pred = rf.predict(X_tf199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       bruit       1.00      0.95      0.97     98999\n",
      "     adresse       0.15      0.90      0.26      1000\n",
      "\n",
      "    accuracy                           0.95     99999\n",
      "   macro avg       0.58      0.92      0.62     99999\n",
      "weighted avg       0.99      0.95      0.97     99999\n",
      "\n",
      "accuracy of the model is : 0.94959949599496\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics #on imprime les métrics d'évaluations\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target_names = ['bruit', 'adresse']\n",
    "print(metrics.classification_report(y, y_pred, target_names=target_names))\n",
    "print(\"accuracy of the model is :\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# load the model from disk\n",
    "\n",
    "gb = pickle.load(open(\"gb.pkl\", \"rb\"))\n",
    "y_pred = gb.predict(X_tf199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       bruit       1.00      0.98      0.99     98999\n",
      "     adresse       0.28      0.96      0.44      1000\n",
      "\n",
      "    accuracy                           0.98     99999\n",
      "   macro avg       0.64      0.97      0.71     99999\n",
      "weighted avg       0.99      0.98      0.98     99999\n",
      "\n",
      "accuracy of the model is : 0.975449754497545\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics #on imprime les métrics d'évaluations\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target_names = ['bruit', 'adresse']\n",
    "print(metrics.classification_report(y, y_pred, target_names=target_names))\n",
    "print(\"accuracy of the model is :\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
